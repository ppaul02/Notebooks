{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CSV_ETL_Transformation_Testing_Notebook", "name": "CSV_ETL_Transformation_Testing_Notebook", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "3a0626a5-0000-0100-0000-62963e4a0000", "properties": {"entityState": null, "renameOperationDetails": null, "targetSparkConfiguration": null, "folder": {"name": "Testing_PoC/CSV"}, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "28g", "driver_cores": 4, "executor_memory": "28g", "executor_cores": 4, "num_executors": 2}, "metadata": {"enableDebugMode": false, "saveOutput": true, "sessionKeepAliveTimeout": 30, "a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": null, "cell_type": "markdown", "metadata": {"nteract": {"transient": {"deleting": false}}}, "source": ["# Import Required Modules"], "outputs": []}, {"execution_count": 2, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#import required modules\r\n", "\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions\r\n", "from pyspark.sql import SparkSession\r\n", "from pyspark.sql.types import *\r\n", "from datetime import date\r\n", "import time"], "outputs": []}, {"execution_count": 3, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#import module to allow paralelism \r\n", "\r\n", "import parsl\r\n", "from parsl.app.app import python_app, bash_app\r\n", "parsl.load()"], "outputs": []}, {"execution_count": null, "cell_type": "markdown", "metadata": {"nteract": {"transient": {"deleting": false}}}, "source": ["# Fetch the parameter values from Pipeline"], "outputs": []}, {"execution_count": 4, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "tags": ["parameters"]}, "source": ["#pull in source & target parameter values from synapse pipeline\r\n", "object_type=\"\"\r\n", "\r\n", "target_object_name='global_superstore'\r\n", "target_account_name = \"etlstoragepocaccount\"\r\n", "target_container_name = \"etlstoragepocfilesystem\"\r\n", "target_relative_path = \"csv\"\r\n", "\r\n", "Run_Environment=\"\"\r\n", "Test_Tag=\"\"\r\n", "\r\n", "sqlservername = 'XXX'\r\n", "sqldbusername = 'XXX'\r\n", "jdbcDatabase = 'XXX'\r\n", "password = 'XXX'"], "outputs": []}, {"execution_count": null, "cell_type": "markdown", "metadata": {"nteract": {"transient": {"deleting": false}}}, "source": ["# Configure The Target Details"], "outputs": []}, {"execution_count": 6, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["jdbcPort = \"1433\"\r\n", "\r\n", "jdbcHostname = \"%s.database.windows.net\" %(sqlservername)\r\n", "username = \"%s@%s\"%(sqldbusername,sqlservername)\r\n", "\r\n", "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n", "connectionProperties = {\r\n", "    \"user\" : username,\r\n", "    \"password\" : password,\r\n", "    \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n", "    }"], "outputs": []}, {"execution_count": 7, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["pushdown_query = \"(Select * from sqldbschema.\"+target_object_name+\") source_object\"\r\n", "df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"], "outputs": []}, {"execution_count": null, "cell_type": "markdown", "metadata": {"nteract": {"transient": {"deleting": false}}}, "source": ["# Configure The Source Details"], "outputs": []}, {"execution_count": 8, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#Get Target DataFrame\r\n", "\r\n", "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (target_container_name, target_account_name, target_relative_path)\r\n", "\r\n", "df1 = spark.read.option('header', 'true') \\\r\n", "                .option('delimiter', ',') \\\r\n", "                .csv(adls_path + '/'+target_object_name+'.csv')"], "outputs": []}, {"execution_count": null, "cell_type": "markdown", "metadata": {"nteract": {"transient": {"deleting": false}}}, "source": ["# Import The Test Case Function Definitions"], "outputs": []}, {"execution_count": 9, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["%run Testing_PoC/CSV/CheckFunctions"], "outputs": []}, {"execution_count": null, "cell_type": "markdown", "metadata": {"nteract": {"transient": {"deleting": false}}}, "source": ["# Configure the Run Environment and Test Tag Details"], "outputs": []}, {"execution_count": 10, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["pre_run_env_cond=\"\"\r\n", "for env in Run_Environment.split(','):\r\n", "    if pre_run_env_cond==\"\":\r\n", "        pre_run_env_cond=\"(lower(Run_Env) like '%\"+env.lower()+\"%')\"\r\n", "    else:\r\n", "        pre_run_env_cond=pre_run_env_cond+\" or (lower(Run_Env) like '%\"+env.lower()+\"%')\"\r\n", "run_env_cond=\"(\"+pre_run_env_cond+\")\"\r\n", "print(run_env_cond)"], "outputs": []}, {"execution_count": 11, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["pre_test_tag_cond=\"\"\r\n", "for tag in Test_Tag.split(','):\r\n", "    if pre_test_tag_cond==\"\":\r\n", "        pre_test_tag_cond=\"(lower(Test_Tag) like '%\"+tag.lower()+\"%')\"\r\n", "    else:\r\n", "        pre_test_tag_cond=pre_test_tag_cond+\" or (lower(Test_Tag) like '%\"+tag.lower()+\"%')\"\r\n", "test_tag_cond=\"(\"+pre_test_tag_cond+\")\"\r\n", "print(test_tag_cond)"], "outputs": []}, {"execution_count": null, "cell_type": "markdown", "metadata": {"nteract": {"transient": {"deleting": false}}}, "source": ["# Run The Test Cases \r\n", "Updates the Transformation Summary and Pipeline Summary Table"], "outputs": []}, {"execution_count": 12, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["#Run the test cases in parallel\r\n", "@python_app\r\n", "def dummy(limit,delay):\r\n", "    from random import randint\r\n", "    return randint(1,limit)\r\n", "    \r\n", "current_date = str(date.today())\r\n", "\r\n", "#getMaximimRunIDfromPipelineSummaryTable for the object\r\n", "run_id=get_run_id(target_object_name,current_date,jdbcUrl,connectionProperties)\r\n", "\r\n", "#get test cases\r\n", "pushdown_query = \"(Select Test_Case,source_column,target_column,additional_arguments from [sqldbschema].[Test_Case_Control] where Layer='Transformation' and Object_Type='csv' and Table_name ='\"+target_object_name+\"'and \"+test_tag_cond+\" and \"+run_env_cond+\") Test_Case_Control\"\r\n", "\r\n", "print(pushdown_query)\r\n", "\r\n", "df_Test_Case = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "\r\n", "print(df_Test_Case.count())\r\n", "\r\n", "columns_TransformationSummary = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "                      StructField(\"Run_Date\", StringType(), False),\r\n", "                      StructField(\"Run_ID\", IntegerType(), False),\r\n", "                      StructField(\"DataTransformation_TestScenario\", StringType(), False),\r\n", "                      StructField(\"Status\", StringType(), False),\r\n", "                      StructField(\"ErrorRecords\", IntegerType(), False),\r\n", "                      StructField(\"Path_To_Failed_Record_Dataset\", StringType(), False)])\r\n", "\r\n", "return_value=\"1\"   \r\n", "\r\n", "for test_case in df_Test_Case.collect():\r\n", "    v_test_case=test_case[0]\r\n", "    v2_test_case=test_case[3]\r\n", "    test_case_count=eval(\"\"+v_test_case+\" (df1 ,df,\"+v2_test_case+\")\")\r\n", "\r\n", "    print(\"\"+v_test_case+\" (df1 ,df,\"+v2_test_case+\")\")\r\n", "\r\n", "    #update TransformationSummary table\r\n", "    v_file_location=str(\"Test_Output/%s/Test_Run_%s/%s\") % (current_date,run_id,v_test_case)\r\n", "    if test_case_count>0:\r\n", "        vals_temp_count=[(target_object_name,current_date,run_id,v_test_case,\"Failed\",test_case_count,v_file_location)]\r\n", "    else:\r\n", "        vals_temp_count=[(target_object_name,current_date,run_id,v_test_case,\"Success\",\"0\",\"\")]\r\n", "\r\n", "    df_temp_test_case = spark.createDataFrame(vals_temp_count, columns_TransformationSummary)\r\n", "\r\n", "    df_temp_test_case.write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Transformation_Summary\", properties=connectionProperties) \r\n", "\r\n", "    if test_case_count>0:\r\n", "        return_value=\"0\" \r\n", "\r\n", "columns_PipelineSummary = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "          StructField(\"Run_Date\", StringType(), False),\r\n", "          StructField(\"Run_ID\", IntegerType(), False),\r\n", "          StructField(\"Layer\", StringType(), False),\r\n", "          StructField(\"Status\", StringType(), False)])\r\n", "\r\n", "#update PipelineSummary table\r\n", "if (return_value==\"1\"):\r\n", "    vals_pipelinesumm=[(target_object_name,current_date,run_id,'Transformation','Success')]\r\n", "    df_temp_pipelinesumm = spark.createDataFrame(vals_pipelinesumm, columns_PipelineSummary) \r\n", "else:\r\n", "    vals_pipelinesumm=[(target_object_name,current_date,run_id,'Transformation','Failed')]\r\n", "    df_temp_pipelinesumm = spark.createDataFrame(vals_pipelinesumm, columns_PipelineSummary) \r\n", "\r\n", "df_temp_pipelinesumm.write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Pipeline_Summary\", properties=connectionProperties)   "], "outputs": []}, {"execution_count": 13, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#return exit value to synapse pipeline\r\n", "mssparkutils.notebook.exit(return_value)"], "outputs": []}]}}