{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CSV_ETL_Ingestion_Notebook_v4", "name": "CSV_ETL_Ingestion_Notebook_v4", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "0b000c2d-0000-0100-0000-626bf79c0000", "properties": {"folder": {"name": "Testing_PoC/CSV/DNU"}, "targetSparkConfiguration": null, "entityState": null, "renameOperationDetails": null, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "28g", "driver_cores": 4, "executor_memory": "28g", "executor_cores": 4, "num_executors": 2}, "metadata": {"a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "saveOutput": true, "sessionKeepAliveTimeout": 30, "enableDebugMode": false, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": 13, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#import required modules\r\n", "\r\n", "import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T\r\n", "from pyspark.sql import SparkSession\r\n", "from pyspark.sql.types import *"], "outputs": []}, {"execution_count": 14, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#pull in source parameter values from KEY VAULT\r\n", "\r\n", "sqlservername = mssparkutils.credentials.getSecret('DataQualityPOCKeyVault','sqlservername')\r\n", "sqldbusername = mssparkutils.credentials.getSecret('DataQualityPOCKeyVault','sqldatabaseusername')\r\n", "jdbcDatabase = mssparkutils.credentials.getSecret('DataQualityPOCKeyVault','sqldatabase')\r\n", "password = mssparkutils.credentials.getSecret('DataQualityPOCKeyVault','sqldbpassword')\r\n", "\r\n", "jdbcPort = \"1433\"\r\n", "\r\n", "jdbcHostname = \"%s.database.windows.net\" %(sqlservername)\r\n", "username = \"%s@%s\"%(sqldbusername,sqlservername)\r\n", "\r\n", "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n", "connectionProperties = {\r\n", "    \"user\" : username,\r\n", "    \"password\" : password,\r\n", "    \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n", "    }"], "outputs": []}, {"execution_count": 15, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#pull in target parameter values from synapse pipeline\r\n", "object_type='sql'\r\n", "\r\n", "target_object_name='global_superstore'\r\n", "target_account_name = \"etlstoragepocaccount\"\r\n", "target_container_name = \"etlstoragepocfilesystem\"\r\n", "target_relative_path = \"csv\"\r\n", "\r\n", "source_account_name = \"etlstoragepocaccount\"\r\n", "source_container_name = \"etlstoragepocfilesystem\"\r\n", "source_relative_path = \"csv\"\r\n", "source_object_name='srcsqldbschema.Global_Superstore'\r\n", ""], "outputs": []}, {"execution_count": 16, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#Get Target DataFrame\r\n", "\r\n", "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (target_container_name, target_account_name, target_relative_path)\r\n", "\r\n", "globals()['df1_%s'%target_object_name] = spark.read.option('header', 'true') \\\r\n", "                .option('delimiter', ',') \\\r\n", "                .csv(adls_path + '/'+target_object_name+'.csv')\r\n", "\r\n", "globals()['df1_%s'%target_object_name].createOrReplaceGlobalTempView(\"df1_view_\"+target_object_name)"], "outputs": []}, {"execution_count": 17, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#Get Source DataFrame\r\n", "if object_type=='sql':\r\n", "  pushdown_query = \"(Select * from \"+source_object_name+\") source_object\"\r\n", "  globals()['df_%s'%target_object_name] = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "\r\n", "if object_type=='parquet':\r\n", "  adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (source_container_name, source_account_name, source_relative_path)\r\n", "  globals()['df1_%s'%target_object_name] = spark.read.load(adls_path + '/'+source_object_name+'/', format='parquet')\r\n", "\r\n", "\r\n", "globals()['df1_%s'%target_object_name].createOrReplaceGlobalTempView(\"df_view_\"+target_object_name)"], "outputs": []}, {"execution_count": 18, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["%run Testing_PoC/CSV/CheckFunctions"], "outputs": []}, {"execution_count": 19, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#condition checks for Source\r\n", "\r\n", "#get current date\r\n", "from datetime import date\r\n", "current_date = str(date.today())\r\n", "\r\n", "#getMaximimRunIDfromPipelineSummaryTable for the object\r\n", "globals()['pushdown_query_%s'%target_object_name] = \"(Select * from sqldbschema.Pipeline_Summary) Pipeline_Summary\"\r\n", "globals()['df_pipleline_summ_%s'%target_object_name] = spark.read.jdbc(url=jdbcUrl, table=globals()['pushdown_query_%s'%target_object_name], properties=connectionProperties)\r\n", "globals()['df_pipleline_summ_%s'%target_object_name].registerTempTable('Pipeline_Summary_'+target_object_name+\"\")\r\n", "globals()['run_id_%s'%target_object_name]=spark.sql(\"select ifnull(max(run_id)+1,1) from Pipeline_Summary_\"+target_object_name+\" where Table_Name ='\"+target_object_name+\"' and run_date='\"+current_date+\"'\").first()[0]\r\n", "\r\n", "#get test cases\r\n", "globals()['pushdown_query_%s'%target_object_name] = \"(Select Test_Case from [sqldbschema].[Test_Case_Control] where Layer='Ingestion' and Object_Type='\"+object_type+\"' and Table_name ='\"+target_object_name+\"') Test_Case_Control\"\r\n", "globals()['df_Test_Case_%s'%target_object_name] = spark.read.jdbc(url=jdbcUrl, table=globals()['pushdown_query_%s'%target_object_name], properties=connectionProperties)\r\n", "\r\n", "columns_IngestionSummary = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "                      StructField(\"Run_Date\", StringType(), False),\r\n", "                      StructField(\"Run_ID\", IntegerType(), False),\r\n", "                      StructField(\"Check_Type\", StringType(), False),\r\n", "                      StructField(\"Status\", StringType(), False)]) \r\n", "\r\n", "globals()['return_value_%s'%target_object_name]=\"1\"   \r\n", "\r\n", "for globals()['test_case_%s'%target_object_name] in globals()['df_Test_Case_%s'%target_object_name].collect():\r\n", "    globals()['v_test_case_%s'%target_object_name]=globals()['test_case_%s'%target_object_name][0]\r\n", "    globals()['test_case_flag_%s'%target_object_name]=eval(\"\"+globals()['v_test_case_%s'%target_object_name]+\"('df_view_\"+target_object_name+\"','df1_view_\"+target_object_name+\"')\")\r\n", "        \r\n", "    #update IngestionSummary table\r\n", "    globals()['vals_temp_count_%s'%target_object_name]=[(target_object_name,current_date,globals()['run_id_%s'%target_object_name],globals()['v_test_case_%s'%target_object_name],globals()['test_case_flag_%s'%target_object_name])]\r\n", "    globals()['df_temp_test_case_%s'%target_object_name] = spark.createDataFrame(globals()['vals_temp_count_%s'%target_object_name], columns_IngestionSummary)\r\n", "\r\n", "    globals()['df_temp_test_case_%s'%target_object_name].write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Ingestion_Summary\", properties=connectionProperties) \r\n", "\r\n", "    if globals()['test_case_flag_%s'%target_object_name]!='Success':\r\n", "        globals()['return_value_%s'%target_object_name]=\"0\" \r\n", "\r\n", "columns_PipelineSummary = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "          StructField(\"Run_Date\", StringType(), False),\r\n", "          StructField(\"Run_ID\", IntegerType(), False),\r\n", "          StructField(\"Layer\", StringType(), False),\r\n", "          StructField(\"Status\", StringType(), False)])\r\n", "\r\n", "#update PipelineSummary table\r\n", "if (globals()['return_value_%s'%target_object_name]==\"1\"):\r\n", "    globals()['vals_pipelinesumm_%s'%target_object_name]=[(target_object_name,current_date,globals()['run_id_%s'%target_object_name],'Ingestion','Success')]\r\n", "    globals()['df_temp_pipelinesumm_%s'%target_object_name] = spark.createDataFrame(globals()['vals_pipelinesumm_%s'%target_object_name], columns_PipelineSummary) \r\n", "else:\r\n", "    globals()['vals_pipelinesumm_%s'%target_object_name]=[(target_object_name,current_date,globals()['run_id_%s'%target_object_name],'Ingestion','Failed')]\r\n", "    globals()['df_temp_pipelinesumm_%s'%target_object_name] = spark.createDataFrame(globals()['vals_pipelinesumm_%s'%target_object_name], columns_PipelineSummary) \r\n", "\r\n", "globals()['df_temp_pipelinesumm_%s'%target_object_name].write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Pipeline_Summary\", properties=connectionProperties)   \r\n", ""], "outputs": []}, {"execution_count": 20, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#return exit value to synapse pipeline\r\n", "mssparkutils.notebook.exit(globals()['return_value_%s'%target_object_name])"], "outputs": []}]}}