{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/ETL_Transformation_Simulate", "name": "ETL_Transformation_Simulate", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "3a06e1ac-0000-0100-0000-62963e710000", "properties": {"entityState": null, "renameOperationDetails": null, "targetSparkConfiguration": null, "folder": {"name": "Testing_PoC/CSV"}, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "28g", "driver_cores": 4, "executor_memory": "28g", "executor_cores": 4, "num_executors": 2}, "metadata": {"enableDebugMode": false, "saveOutput": true, "sessionKeepAliveTimeout": 30, "a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": 2, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#import required modules\r\n", "\r\n", "import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T\r\n", "from pyspark.sql import SparkSession\r\n", "from pyspark.sql.types import *"], "outputs": []}, {"execution_count": 6, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "tags": ["parameters"]}, "source": ["#pull in target parameter values from synapse pipeline\r\n", "\r\n", "target_object_name='global_superstore_parquet'\r\n", "target_account_name = \"etlstoragepocaccount\"\r\n", "target_container_name = \"etlstoragepocfilesystem\"\r\n", "target_relative_path = \"csv\"\r\n", "object_type='parquet'\r\n", "\r\n", "sqlservername = 'XXX'\r\n", "sqldbusername = 'XXX'\r\n", "jdbcDatabase = 'XXX'\r\n", "password = 'XXX'"], "outputs": []}, {"execution_count": 7, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["object_type = \"parquet\"\r\n", "target_object_name = \"global_superstore_parquet\"\r\n", "target_container_name = \"etlstoragepocfilesystem\"\r\n", "target_relative_path = \"csv\"\r\n", "sqlservername = \"sqlserverdataqualitypoc\"\r\n", "sqldbusername = \"serveradminlogindatadqualitypoc\"\r\n", "jdbcDatabase = \"sqldbdataqualitypoc\"\r\n", "password = \"@1234Password\"\r\n", "Run_Environment = 'dev'\r\n", "Test_Tag = 'Unit'"], "outputs": []}, {"execution_count": 8, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["jdbcPort = \"1433\"\r\n", "\r\n", "jdbcHostname = \"%s.database.windows.net\" %(sqlservername)\r\n", "username = \"%s@%s\"%(sqldbusername,sqlservername)\r\n", "\r\n", "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n", "connectionProperties = {\r\n", "    \"user\" : username,\r\n", "    \"password\" : password,\r\n", "    \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n", "    }"], "outputs": []}, {"execution_count": 9, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#Get raw file DataFrame\r\n", "\r\n", "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (target_container_name, target_account_name, target_relative_path)\r\n", "\r\n", "df1 = spark.read.option('header', 'true') \\\r\n", "                .option('delimiter', ',') \\\r\n", "                .csv(adls_path + '/'+target_object_name+'.csv')\r\n", ""], "outputs": []}, {"execution_count": 10, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["df1.registerTempTable('GlobalSuperstore')"], "outputs": []}, {"execution_count": 11, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["from pyspark.sql.functions import *\r\n", "\r\n", "if object_type=='sql':\r\n", "    df2=df1.selectExpr(\"*\",\"CASE WHEN `Order Priority` == 'Critical' THEN  1 WHEN `Order Priority` == 'High' THEN  2 WHEN `Order Priority`== 'Medium' THEN 3  ELSE 4 END AS `Order_Priority_Derived`\")\r\n", "    df3=df2.selectExpr(\"*\",\"`Sales`/`Quantity` AS `Sales_Per_Unit`\")\r\n", "    df4=df3.select(\"*\",col(\"Order Date\"),date_format(to_date(col(\"Order Date\"),'dd/MM/yyyy'),\"yyyy-MM-dd\").alias(\"Order_Date\")) \r\n", "    df1=df4.selectExpr(\"`Row ID` as `Row_ID`\",\"`Order ID` as `Order_ID`\",\"`Order_Date` as `Order_Date`\",\"`Sales_Per_Unit` as `Sales_Per_Unit`\",\"`Order_Priority_Derived` as `Order_Priority`\")\r\n", ""], "outputs": []}, {"execution_count": 12, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["df1.write.mode('overwrite').jdbc(jdbcUrl,\"sqldbschema.\"+target_object_name, properties=connectionProperties)"], "outputs": []}]}}