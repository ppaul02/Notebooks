{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CSV_ETL_Ingestion_Notebook_bkp", "name": "CSV_ETL_Ingestion_Notebook_bkp", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "0b00f22c-0000-0100-0000-626bf78b0000", "properties": {"folder": {"name": "Testing_PoC/CSV/DNU"}, "targetSparkConfiguration": null, "entityState": null, "renameOperationDetails": null, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "28g", "driver_cores": 4, "executor_memory": "28g", "executor_cores": 4, "num_executors": 2}, "metadata": {"sessionKeepAliveTimeout": 30, "saveOutput": true, "enableDebugMode": false, "a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "synapse_widget": {"version": "0.1"}, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T"], "outputs": []}, {"execution_count": 2, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "tags": ["parameters"]}, "source": ["object_name='global_superstore'"], "outputs": []}, {"execution_count": 3, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["from pyspark.sql import SparkSession\r\n", "from pyspark.sql.types import *\r\n", "account_name = \"etlstoragepocaccount\"\r\n", "container_name = \"etlstoragepocfilesystem\"\r\n", "relative_path = \"csv\"\r\n", "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n", "\r\n", "df1 = spark.read.option('header', 'true') \\\r\n", "                .option('delimiter', ',') \\\r\n", "                .csv(adls_path + '/'+object_name+'.csv')"], "outputs": []}, {"execution_count": 4, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["jdbcHostname = \"sqlserverdataqualitypoc.database.windows.net\"\r\n", "jdbcDatabase = \"sqldbdataqualitypoc\"\r\n", "jdbcPort = \"1433\"\r\n", "username = \"serveradminlogindatadqualitypoc@sqlserverdataqualitypoc\"\r\n", "password = \"@1234Password\"\r\n", "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n", "connectionProperties = {\r\n", "   \"user\" : username,\r\n", "   \"password\" : password,\r\n", "   \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n", " }\r\n", "pushdown_query = \"(Select * from srcsqldbschema.Global_Superstore) Global_Superstore\"\r\n", "df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"], "outputs": []}, {"execution_count": 5, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["# for metadata comparision\r\n", "meta_flag=\"Not Run\"\r\n", "if (df.dtypes==df1.dtypes):\r\n", "    meta_flag=\"Success\"\r\n", "else :\r\n", "    meta_flag=\"Failed\"\r\n", "    \r\n", "print(meta_flag)"], "outputs": []}, {"execution_count": 6, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["# for count comparision\r\n", "count_flag=\"Not Run\"\r\n", "if (df1.count()!=df1.count()):\r\n", "    count_flag==\"Failed\"\r\n", "else :\r\n", "    count_flag=\"Success\"\r\n", "    \r\n", "\r\n", "print(count_flag)"], "outputs": []}, {"execution_count": 7, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["# for null comparision\r\n", "null_flag=\"Not Run\"\r\n", "\r\n", "df_row_count=df.count()\r\n", "df_column_count=len(df.columns)\r\n", "df1_row_count=df1.count()\r\n", "df1_column_count=len(df1.columns)\r\n", "df_null_count=0\r\n", "df1_null_count=0\r\n", "\r\n", "i=0\r\n", "j=0\r\n", "\r\n", "if (df_column_count==df1_column_count and df1_row_count==df1_row_count):\r\n", "    while i < df_row_count:\r\n", "        while j < df_column_count:\r\n", "            if df[i][j] is None:\r\n", "                df_null_count+=1\r\n", "            if df1[i][j] is None:\r\n", "                df1_null_count+=1\r\n", "            j+=1\r\n", "        i+=1\r\n", "    if(df_null_count==df1_null_count):\r\n", "        null_flag=\"Success\"\r\n", "else:\r\n", "    null_flag=\"Failed\"\r\n", "\r\n", "print(null_flag)    \r\n", ""], "outputs": []}, {"execution_count": 8, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["from datetime import date\r\n", "current_date = str(date.today())"], "outputs": []}, {"execution_count": 9, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["#getMaximimRunIDfromPipelineSummaryTable\r\n", "\r\n", "pushdown_query = \"(Select * from sqldbschema.Pipeline_Summary) Pipeline_Summary\"\r\n", "df_pipleline_summ = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "df_pipleline_summ.registerTempTable('Pipeline_Summary')\r\n", "run_id=spark.sql(\"select ifnull(max(run_id)+1,1) from Pipeline_Summary where run_date='\"+current_date+\"'\").first()[0]\r\n", "print(run_id)"], "outputs": []}, {"execution_count": 10, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["columns = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "                      StructField(\"Run_Date\", StringType(), False),\r\n", "                      StructField(\"Run_ID\", IntegerType(), False),\r\n", "                      StructField(\"Check_Type\", StringType(), False),\r\n", "                      StructField(\"Status\", StringType(), False)])\r\n", "                      \r\n", "                    \r\n", "vals_temp_count=[('global_superstore',current_date,run_id,'Count Check',count_flag)]\r\n", "df_temp_count = spark.createDataFrame(vals_temp_count, columns) \r\n", "vals_temp_null=[('global_superstore',current_date,run_id,'Null Check',null_flag)]\r\n", "df_temp_null = spark.createDataFrame(vals_temp_null, columns) \r\n", "vals_temp_meta=[('global_superstore',current_date,run_id,'Metadata Check',meta_flag)]\r\n", "df_temp_meta = spark.createDataFrame(vals_temp_meta, columns) \r\n", "df_ingestionsummary=df_temp_count.union(df_temp_null).union(df_temp_meta)"], "outputs": []}, {"execution_count": 11, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["columns = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "                      StructField(\"Run_Date\", StringType(), False),\r\n", "                      StructField(\"Run_ID\", IntegerType(), False),\r\n", "                      StructField(\"Layer\", StringType(), False),\r\n", "                      StructField(\"Status\", StringType(), False)])\r\n", "\r\n", "if (meta_flag=='Success' and null_flag=='Success' and count_flag=='Success'):\r\n", "    vals_pipelinesumm=[('global_superstore',current_date,run_id,'Raw','Success')]\r\n", "    df_temp_pipelinesumm = spark.createDataFrame(vals_pipelinesumm, columns) \r\n", "else:\r\n", "     vals_pipelinesumm=[('global_superstore',current_date,run_id,'Raw','Failed')]\r\n", "     df_temp_pipelinesumm = spark.createDataFrame(vals_pipelinesumm, columns) "], "outputs": []}, {"execution_count": 12, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["df_temp_pipelinesumm.write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Pipeline_Summary\", properties=connectionProperties)"], "outputs": []}, {"execution_count": 13, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["df_ingestionsummary.write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Ingestion_Summary\", properties=connectionProperties)"], "outputs": []}, {"execution_count": 14, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["if (meta_flag=='Success' and null_flag=='Success' and count_flag=='Success'):\r\n", "    return_value=\"1\"\r\n", "else:\r\n", "    return_value=\"0\"\r\n", "\r\n", "mssparkutils.notebook.exit(return_value)"], "outputs": []}]}}