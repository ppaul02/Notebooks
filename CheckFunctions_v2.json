{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CheckFunctions_v2", "name": "CheckFunctions_v2", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "0b00ab2d-0000-0100-0000-626bf7db0000", "properties": {"folder": {"name": "Testing_PoC/CSV/DNU"}, "targetSparkConfiguration": null, "entityState": null, "renameOperationDetails": null, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "112g", "driver_cores": 16, "executor_memory": "112g", "executor_cores": 16, "num_executors": 2}, "metadata": {"a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "saveOutput": true, "sessionKeepAliveTimeout": 30, "enableDebugMode": false, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": 3, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T"], "outputs": []}, {"execution_count": 18, "cell_type": "code", "source": ["def metadata_check (df,df1):\r\n", "    global_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\")\r\n", "    df1_meta=spark.sql(\"select * from \" +global_temp_db + \".\" + df1)\r\n", "    df_meta=spark.sql(\"select * from \" +global_temp_db + \".\" + df)\r\n", "\r\n", "    meta_flag=\"Not Run\"\r\n", "\r\n", "    if (df_meta.columns==df1_meta.columns):\r\n", "        meta_flag=\"Success\"\r\n", "    else :\r\n", "        meta_flag=\"Failed\"\r\n", "\r\n", "    return(meta_flag)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def count_check (df,df1):\r\n", "    global_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\")\r\n", "    df1_count=spark.sql(\"select * from \" +global_temp_db + \".\" + df1)\r\n", "    df_count=spark.sql(\"select * from \" +global_temp_db + \".\" + df)\r\n", "    count_flag=\"Not Run\"\r\n", "    if (df1_count.count()!=df1_count.count()):\r\n", "        count_flag==\"Failed\"\r\n", "    else :\r\n", "        count_flag=\"Success\"\r\n", "\r\n", "    return(count_flag)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def null_check (df,df1):\r\n", "    global_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\")\r\n", "    df1_null=spark.sql(\"select * from \" +global_temp_db + \".\" + df1)\r\n", "    df_null=spark.sql(\"select * from \" +global_temp_db + \".\" + df)\r\n", "    null_flag=\"Not Run\"\r\n", "    \r\n", "    df_row_count=df_null.count()\r\n", "    df_column_count=len(df_null.columns)\r\n", "    df1_row_count=df1_null.count()\r\n", "    df1_column_count=len(df1_null.columns)\r\n", "    df_null_count=0\r\n", "    df1_null_count=0\r\n", "    \r\n", "    i=0\r\n", "    j=0\r\n", "    \r\n", "\r\n", "    if (df_column_count==df1_column_count and df1_row_count==df_row_count):\r\n", "        while i < df_row_count:\r\n", "            while j < df_column_count:\r\n", "                if df_null[i][j] is None:\r\n", "                    df_null_count+=1\r\n", "                if df1_null[i][j] is None:\r\n", "                    df1_null_count+=1\r\n", "                j+=1\r\n", "            i+=1\r\n", "            \r\n", "    if(df_null_count==df1_null_count):\r\n", "        null_flag=\"Success\"\r\n", "    else:\r\n", "        null_flag=\"Failed\"\r\n", "\r\n", "    return(null_flag)"], "outputs": []}]}}