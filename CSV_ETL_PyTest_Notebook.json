{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CSV_ETL_PyTest_Notebook", "name": "CSV_ETL_PyTest_Notebook", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "3a06b89e-0000-0100-0000-62963e2b0000", "properties": {"entityState": null, "renameOperationDetails": null, "targetSparkConfiguration": null, "folder": {"name": "Testing_PoC/CSV/DNU."}, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "28g", "driver_cores": 4, "executor_memory": "28g", "executor_cores": 4, "num_executors": 2}, "metadata": {"enableDebugMode": false, "saveOutput": true, "sessionKeepAliveTimeout": 30, "a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": 68, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#import required modules\r\n", "\r\n", "import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T\r\n", "from pyspark.sql import SparkSession\r\n", "from pyspark.sql.types import *\r\n", "from pyspark.sql.functions import when, lit, col"], "outputs": []}, {"execution_count": 69, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["%run Testing_PoC/CSV/CheckFunctions"], "outputs": []}, {"execution_count": 70, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["def validate_count_check():\r\n", "  schema = StructType([StructField(\"firstname\",StringType(),True),\r\n", "           StructField(\"middlename\",StringType(),True),\r\n", "           StructField(\"lastname\",StringType(),True),\r\n", "           StructField(\"id\", StringType(), True),\r\n", "           StructField(\"gender\", StringType(), True),\r\n", "           ])\r\n", "  data1 = [(\"ABC1\",\"\",\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABC2\",\"\",\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABC3\",\"\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABC4\",\"\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABC5\",\"\",\"XYZ5\",\"5\",\"M\"),\r\n", "           ]\r\n", "\r\n", "  data2 = [(\"ABCD1\",\"\",\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABCD2\",\"\",\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABCD3\",\"\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABCD4\",\"\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABCD5\",\"\",\"XYZ5\",\"5\",\"M\"),\r\n", "           ]\r\n", "  data3 = [(\"ABCD1\",\"\",\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABCD2\",\"\",\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABCD3\",\"\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABCD4\",\"\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABCD5\",\"\",\"XYZ5\",\"5\",\"M\"),\r\n", "           (\"ABCD6\",\"\",\"XYZ6\",\"6\",\"M\"),\r\n", "           ]\r\n", "\r\n", "  df = spark.createDataFrame(data=data1,schema=schema)\r\n", "  df1 = spark.createDataFrame(data=data2,schema=schema)\r\n", "  df2 = spark.createDataFrame(data=data3,schema=schema)\r\n", "\r\n", "  assert count_check(df,df1)=='Success',\"COUNT_CHECK has incorrect function definition:Case1\"\r\n", "  assert count_check(df,df2)=='Failed',\"COUNT_CHECK has incorrect function definition:Case2\""], "outputs": []}, {"execution_count": 71, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["\r\n", "def validate_metadata_check():\r\n", "  schema = StructType([StructField(\"firstname\",StringType(),True),\r\n", "           StructField(\"middlename\",StringType(),True),\r\n", "           StructField(\"lastname\",StringType(),True),\r\n", "           StructField(\"id\", StringType(), True),\r\n", "           StructField(\"gender\", StringType(), True),\r\n", "           ])\r\n", "  data1 = [(\"ABC1\",\"\",\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABC2\",\"\",\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABC3\",\"\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABC4\",\"\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABC5\",\"\",\"XYZ5\",\"5\",\"M\"),\r\n", "           ]\r\n", "\r\n", "  data2 = [(\"ABCD1\",\"\",\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABCD2\",\"\",\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABCD3\",\"\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABCD4\",\"\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABCD5\",\"\",\"XYZ5\",\"5\",\"M\"),\r\n", "           ]\r\n", "  \r\n", "  schema2 = StructType([StructField(\"firstname\",StringType(),True),\r\n", "           StructField(\"middlename\",StringType(),True),\r\n", "           StructField(\"lastname\",StringType(),True),\r\n", "           StructField(\"id\", StringType(), True)\r\n", "           ])\r\n", "\r\n", "\r\n", "\r\n", "  data3 = [(\"ABCD1\",\"\",\"XYZ1\",\"1\"),\r\n", "           (\"ABCD2\",\"\",\"XYZ2\",\"2\"),\r\n", "           (\"ABCD3\",\"\",\"XYZ3\",\"3\"),\r\n", "           (\"ABCD4\",\"\",\"XYZ4\",\"4\"),\r\n", "           (\"ABCD5\",\"\",\"XYZ5\",\"5\"),\r\n", "           (\"ABCD6\",\"\",\"XYZ6\",\"6\"),\r\n", "           ]\r\n", "\r\n", "  df = spark.createDataFrame(data=data1,schema=schema)\r\n", "  df1 = spark.createDataFrame(data=data2,schema=schema)\r\n", "  df2 = spark.createDataFrame(data=data3,schema=schema2)\r\n", "\r\n", "  assert metadata_check(df,df1)=='Success',\"METADATA_CHECK has incorrect function definition:Case1\"\r\n", "  assert metadata_check(df,df2)=='Failed',\"METADATA_CHECK has incorrect function definition:Case2\""], "outputs": []}, {"execution_count": 72, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["def validate_null_check():\r\n", "  schema = StructType([StructField(\"firstname\",StringType(),True),\r\n", "           StructField(\"middlename\",StringType(),True),\r\n", "           StructField(\"lastname\",StringType(),True),\r\n", "           StructField(\"id\", StringType(), True),\r\n", "           StructField(\"gender\", StringType(), True),\r\n", "           ])\r\n", "  data1 = [(\"ABC1\",None,\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABC2\",\"A\",\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABC3\",\"A\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABC4\",\"A\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABC5\",\"A\",\"XYZ5\",\"5\",\"M\"),\r\n", "           ]\r\n", "\r\n", "  data2 = [(\"ABCD1\",None,\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABCD2\",\"A\",\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABCD3\",\"A\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABCD4\",\"A\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABCD5\",\"A\",\"XYZ5\",\"5\",\"M\"),\r\n", "           ]\r\n", "  data3 = [(\"ABCD1\",None,\"XYZ1\",\"1\",\"M\"),\r\n", "           (\"ABCD2\",None,\"XYZ2\",\"2\",\"M\"),\r\n", "           (\"ABCD3\",\"A\",\"XYZ3\",\"3\",\"M\"),\r\n", "           (\"ABCD4\",\"A\",\"XYZ4\",\"4\",\"M\"),\r\n", "           (\"ABCD5\",\"A\",\"XYZ5\",\"5\",\"M\"),\r\n", "           (\"ABCD6\",\"A\",\"XYZ6\",\"6\",\"M\")\r\n", "           ]\r\n", "\r\n", "  df = spark.createDataFrame(data=data1,schema=schema)\r\n", "  df1 = spark.createDataFrame(data=data2,schema=schema)\r\n", "  df2 = spark.createDataFrame(data=data3,schema=schema)\r\n", "\r\n", "\r\n", "  assert null_check(df,df1)=='Success',\"NULL_CHECK has incorrect function definition:Case1\"\r\n", "  assert null_check(df,df2)=='Failed',\"NULL_CHECK has incorrect function definition:Case2\""], "outputs": []}, {"execution_count": 73, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["validate_count_check()\r\n", "validate_metadata_check()\r\n", "validate_null_check()"], "outputs": []}]}}