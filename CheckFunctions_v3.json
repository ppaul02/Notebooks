{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CheckFunctions_v3", "name": "CheckFunctions_v3", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "3a066fa9-0000-0100-0000-62963e600000", "properties": {"entityState": null, "renameOperationDetails": null, "targetSparkConfiguration": null, "folder": {"name": "Testing_PoC/CSV/DNU."}, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "112g", "driver_cores": 16, "executor_memory": "112g", "executor_cores": 16, "num_executors": 2}, "metadata": {"enableDebugMode": false, "saveOutput": true, "sessionKeepAliveTimeout": 30, "a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T"], "outputs": []}, {"execution_count": 18, "cell_type": "code", "source": ["def metadata_check (df,df1):\r\n", "    meta_flag=\"Not Run\"\r\n", "\r\n", "    if (df.columns==df1.columns):\r\n", "        meta_flag=\"Success\"\r\n", "    else :\r\n", "        meta_flag=\"Failed\"\r\n", "\r\n", "    return(meta_flag)\r\n", "\r\n", "\r\n", "\r\n", "def count_check (df_count,df1_count):\r\n", "    count_flag=\"Not Run\"\r\n", "    if (df1_count.count()!=df1_count.count()):\r\n", "        count_flag==\"Failed\"\r\n", "    else :\r\n", "        count_flag=\"Success\"\r\n", "\r\n", "    return(count_flag)\r\n", "\r\n", "\r\n", "\r\n", "def null_check (df_null,df1_null):\r\n", "    null_flag=\"Not Run\"\r\n", "    \r\n", "    df_row_count=df_null.count()\r\n", "    df_column_count=len(df_null.columns)\r\n", "    df1_row_count=df1_null.count()\r\n", "    df1_column_count=len(df1_null.columns)\r\n", "    df_null_count=0\r\n", "    df1_null_count=0\r\n", "    \r\n", "    i=0\r\n", "    j=0\r\n", "    \r\n", "\r\n", "    if (df_column_count==df1_column_count and df1_row_count==df_row_count):\r\n", "        while i < df_row_count:\r\n", "            while j < df_column_count:\r\n", "                if df_null[i][j] is None:\r\n", "                    df_null_count+=1\r\n", "                if df1_null[i][j] is None:\r\n", "                    df1_null_count+=1\r\n", "                j+=1\r\n", "            i+=1\r\n", "            \r\n", "    if(df_null_count==df1_null_count):\r\n", "        null_flag=\"Success\"\r\n", "    else:\r\n", "        null_flag=\"Failed\"\r\n", "\r\n", "    return(null_flag)\r\n", "\r\n", "\r\n", "def get_run_id(target_object_name,current_date,jdbcUrl,connectionProperties):\r\n", "    pushdown_query = \"(Select * from sqldbschema.Pipeline_Summary) Pipeline_Summary\"\r\n", "    df_pipleline_summ = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "    df_pipleline_summ.registerTempTable('Pipeline_Summary_'+target_object_name+\"\")\r\n", "    run_id=spark.sql(\"select ifnull(max(run_id)+1,1) from Pipeline_Summary_\"+target_object_name+\" where Table_Name ='\"+target_object_name+\"' and run_date='\"+current_date+\"'\").first()[0]\r\n", "\r\n", "    return(run_id)\r\n", "\r\n", "\r\n", "def get_test_cases(object_type,target_object_name,jdbcUrl,connectionProperties):\r\n", "    pushdown_query = \"(Select Test_Case from [sqldbschema].[Test_Case_Control] where Layer='Ingestion' and Object_Type='\"+object_type+\"' and Table_name ='\"+target_object_name+\"') Test_Case_Control\"\r\n", "    df_Test_Case = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "\r\n", "    display(df_Test_Case)"], "outputs": []}]}}