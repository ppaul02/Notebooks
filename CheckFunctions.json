{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CheckFunctions", "name": "CheckFunctions", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "3a065ba7-0000-0100-0000-62963e560000", "properties": {"entityState": null, "renameOperationDetails": null, "targetSparkConfiguration": null, "folder": {"name": "Testing_PoC/CSV"}, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "112g", "driver_cores": 16, "executor_memory": "112g", "executor_cores": 16, "num_executors": 2}, "metadata": {"enableDebugMode": false, "saveOutput": true, "sessionKeepAliveTimeout": 30, "a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T\r\n", "from datetime import date\r\n", "from pyspark.sql.functions import *\r\n", ""], "outputs": []}, {"execution_count": 18, "cell_type": "code", "source": ["def metadata_check (df,df1):\r\n", "    meta_flag=\"Not Run\"\r\n", "\r\n", "    if (df.columns==df1.columns):\r\n", "        meta_flag=\"Success\"\r\n", "    else :\r\n", "        meta_flag=\"Failed\"\r\n", "\r\n", "    return(meta_flag)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def count_check (df,df1):\r\n", "    count_flag=\"Not Run\"\r\n", "\r\n", "    if (str(df.count()-df1.count())==\"0\"):\r\n", "        count_flag=\"Success\"\r\n", "    else :\r\n", "        count_flag=\"Failed\"\r\n", "\r\n", "    return(count_flag)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def null_check (df_null,df1_null):\r\n", "    null_flag=\"Not Run\"\r\n", "    \r\n", "\r\n", "    df_column_count=len(df_null.columns)\r\n", "    df1_column_count=len(df1_null.columns)\r\n", "    df_null_count=0\r\n", "    df1_null_count=0\r\n", "\r\n", "    if (df_column_count==df1_column_count):\r\n", "        for row in df_null.collect():\r\n", "            col=0\r\n", "            while col<df_column_count:\r\n", "                if row[col] is None:\r\n", "                    df_null_count=df_null_count+1\r\n", "                col=col+1\r\n", "        \r\n", "        for row in df1_null.collect():\r\n", "            col=0\r\n", "            while col<df1_column_count:\r\n", "                if row[col] is None:\r\n", "                    df1_null_count=df1_null_count+1\r\n", "                col=col+1\r\n", "\r\n", "            \r\n", "    if(df_null_count==df1_null_count):\r\n", "        null_flag=\"Success\"\r\n", "    else:\r\n", "        null_flag=\"Failed\"\r\n", "\r\n", "    return(null_flag)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def get_run_id(target_object_name,current_date,jdbcUrl,connectionProperties):\r\n", "    pushdown_query = \"(Select * from sqldbschema.Pipeline_Summary) Pipeline_Summary\"\r\n", "    df_pipleline_summ = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "    df_pipleline_summ.registerTempTable('Pipeline_Summary_'+target_object_name+\"\")\r\n", "    run_id=spark.sql(\"select ifnull(max(run_id)+1,1) from Pipeline_Summary_\"+target_object_name+\" where Table_Name ='\"+target_object_name+\"' and run_date='\"+current_date+\"'\").first()[0]\r\n", "\r\n", "    return(run_id)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def get_test_cases(object_type,target_object_name,jdbcUrl,connectionProperties):\r\n", "    pushdown_query = \"(Select Test_Case from [sqldbschema].[Test_Case_Control] where Layer='Ingestion' and Object_Type='\"+object_type+\"' and Table_name ='\"+target_object_name+\"') Test_Case_Control\"\r\n", "    df_Test_Case = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "\r\n", "    display(df_Test_Case)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def order_date_transformation (df1,df,run_id,container_name,account_name):\r\n", "    try:\r\n", "        df1=df1.withColumnRenamed('Row_ID', 'Row_ID_CSV')\r\n", "    except:\r\n", "        try_val=1\r\n", "    try:\r\n", "        df1=df1.withColumnRenamed('Row ID', 'Row_ID_CSV')\r\n", "    except:\r\n", "        try_val=2\r\n", "    try:\r\n", "        df1=df1.withColumnRenamed('Order_Date', 'Order Date')\r\n", "    except:\r\n", "        try_val=3\r\n", "    df4=df1.join(df,df.Row_ID ==  df1.Row_ID_CSV,\"left\")\r\n", "    df6=df4.withColumn(\"Order_Date_Test\", when(isnull(col(\"Order Date\")) ,\"1900-01-01\").otherwise(date_format(to_date(col(\"Order Date\"),'dd/MM/yyyy'),\"yyyy-MM-dd\")))\r\n", "    df_combined_order_date = df6.filter((df6.Order_Date).eqNullSafe(df6.Order_Date_Test)==False).select(\"Row_ID\",\"Order_Date\")\r\n", "    order_date_error_count=df_combined_order_date.count()\r\n", "    current_date=str(date.today())\r\n", "    order_date_file_location=\"abfss://%s@%s.dfs.core.windows.net/Test_Output/%s/Test_Run_%s/order_date_transformation\" % (container_name, account_name,current_date,run_id)\r\n", "    if order_date_error_count>0:\r\n", "        df_combined_order_date.coalesce(1).write.format(\"csv\").mode(\"append\").option(\"header\",\"true\").option(\"delimiter\",\"|\").save(order_date_file_location)\r\n", "\r\n", "    return(order_date_error_count)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def sales_data_transformation (df1,df,run_id,container_name,account_name):\r\n", "    try:\r\n", "        df1=df1.withColumnRenamed('Row ID', 'Row_ID_CSV')\r\n", "    except:\r\n", "        df1=df1.withColumnRenamed('Row_ID', 'Row_ID_CSV')\r\n", "    df4=df1.join(df,df.Row_ID ==  df1.Row_ID_CSV,\"left\")\r\n", "    df6=df4.selectExpr(\"*\",\"bround(`Sales`/`Quantity`,2) AS `Sales_Per_Unit_Test`\")\r\n", "    df_combined_sales_pu = df6.filter(df6.Sales_Per_Unit!=df6.Sales_Per_Unit_Test).select(\"Row_ID\",\"Order_ID\",\"Order_Date\",\"Sales_Per_Unit\",\"Order_Priority\")\r\n", "    sales_pu_error_count = df_combined_sales_pu.count()\r\n", "    current_date=str(date.today())\r\n", "    sales_pu_file_location=\"abfss://%s@%s.dfs.core.windows.net/Test_Output/%s/Test_Run_%s/sales_data_transformation\" % (container_name, account_name,current_date,run_id)\r\n", "    if sales_pu_error_count>0:\r\n", "        df_combined_sales_pu.coalesce(1).write.format(\"csv\").mode(\"append\").option(\"header\",\"true\").option(\"delimiter\",\"|\").save(sales_pu_file_location)\r\n", "    \r\n", "    return(sales_pu_error_count)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["def order_priority_transformation (df1,df,run_id,container_name,account_name):\r\n", "    df1=df1.withColumnRenamed('Row ID', 'Row_ID_CSV')\r\n", "    df4=df1.join(df,df.Row_ID ==  df1.Row_ID_CSV,\"left\")\r\n", "    df6=df4.selectExpr(\"*\",\"CASE WHEN `Order Priority` == 'Critical' THEN  0 WHEN `Order Priority` == 'High' THEN  2 WHEN `Order Priority`== 'Medium' THEN 3  ELSE 4 END AS `Order_Priority_Derived_Test`\")\r\n", "    df_combined_order_priority = df6.filter(df6.Order_Priority!=df6.Order_Priority_Derived_Test).select(\"Row_ID\",\"Order_ID\",\"Order_Date\",\"Sales_Per_Unit\",\"Order_Priority\")\r\n", "    order_priority_error_count = df_combined_order_priority.count()\r\n", "    current_date=str(date.today())\r\n", "    order_priority_file_location=\"abfss://%s@%s.dfs.core.windows.net/Test_Output/%s/Test_Run_%s/order_priority_transformation\" % (container_name, account_name,current_date,run_id)\r\n", "    if order_priority_error_count>0:\r\n", "        df_combined_order_priority.coalesce(1).write.format(\"csv\").mode(\"append\").option(\"header\",\"true\").option(\"delimiter\",\"|\").save(order_priority_file_location)\r\n", " \r\n", "    return(order_priority_error_count)"], "outputs": []}]}}