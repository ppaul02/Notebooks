{"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/notebooks/CSV_ETL_Testing_Notebook_bkp", "name": "CSV_ETL_Testing_Notebook_bkp", "type": "Microsoft.Synapse/workspaces/notebooks", "etag": "0b003c2d-0000-0100-0000-626bf7c10000", "properties": {"folder": {"name": "Testing_PoC/CSV/DNU"}, "targetSparkConfiguration": null, "entityState": null, "renameOperationDetails": null, "big_data_pool": {"type": "BigDataPoolReference", "reference_name": "apachesparkpool"}, "session_properties": {"driver_memory": "28g", "driver_cores": 4, "executor_memory": "28g", "executor_cores": 4, "num_executors": 2}, "metadata": {"a365ComputeOptions": {"id": "/subscriptions/0e40d638-6ee2-42fa-83d5-78641df4b3cb/resourceGroups/DataQualityPOC/providers/Microsoft.Synapse/workspaces/synapse-etl-poc/bigDataPools/apachesparkpool", "name": "apachesparkpool", "type": "Spark", "endpoint": "https://synapse-etl-poc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool", "auth": {"type": "AAD", "authResource": "https://dev.azuresynapse.net"}, "sparkVersion": "3.1", "nodeCount": 3, "cores": 16, "memory": 112, "extraHeader": null}, "saveOutput": true, "sessionKeepAliveTimeout": 30, "enableDebugMode": false, "kernelspec": {"name": "synapse_pyspark", "display_name": "Synapse PySpark"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"execution_count": 38, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["import pyodbc\r\n", "import openpyxl\r\n", "from openpyxl import load_workbook\r\n", "from openpyxl.utils.dataframe import dataframe_to_rows\r\n", "from openpyxl.worksheet.table import Table, TableStyleInfo\r\n", "import pandas as pd\r\n", "from pyspark.sql import Row, SparkSession\r\n", "import pyspark.sql.functions as F\r\n", "import pyspark.sql.types as T"], "outputs": []}, {"execution_count": 39, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["from pyspark.sql import SparkSession\r\n", "from pyspark.sql.types import *\r\n", "account_name = \"etlstoragepocaccount\"\r\n", "container_name = \"etlstoragepocfilesystem\"\r\n", "relative_path = \"csv\"\r\n", "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n", "\r\n", "df1 = spark.read.option('header', 'true') \\\r\n", "                .option('delimiter', ',') \\\r\n", "                .csv(adls_path + '/global_superstore.csv')\r\n", "df1=df1.withColumnRenamed('Row ID', 'Row_ID_CSV')"], "outputs": []}, {"execution_count": 40, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["display(df1.limit(5))\r\n", "df1.registerTempTable('GlobalSuperstore')"], "outputs": []}, {"execution_count": 41, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["jdbcHostname = \"sqlserverdataqualitypoc.database.windows.net\"\r\n", "jdbcDatabase = \"sqldbdataqualitypoc\"\r\n", "jdbcPort = \"1433\"\r\n", "username = \"serveradminlogindatadqualitypoc@sqlserverdataqualitypoc\"\r\n", "password = \"@1234Password\"\r\n", "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n", "connectionProperties = {\r\n", "   \"user\" : username,\r\n", "   \"password\" : password,\r\n", "   \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n", " }\r\n", "pushdown_query = \"(Select * from sqldbschema.Global_Superstore) Global_Superstore\"\r\n", "df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "display(df.limit(5))"], "outputs": []}, {"execution_count": 42, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["df4=df1.join(df,df.Row_ID ==  df1.Row_ID_CSV,\"left\")\r\n", "display(df4.limit(5))\r\n", "df4.printSchema()"], "outputs": []}, {"execution_count": 43, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["from pyspark.sql.functions import *\r\n", "\r\n", "df5=df4.selectExpr(\"*\",\"CASE WHEN `Order Priority` == 'Critical' THEN  0 WHEN `Order Priority` == 'High' THEN  2 WHEN `Order Priority`== 'Medium' THEN 3  ELSE 4 END AS `Order_Priority_Derived_Test`\",\"bround(`Sales`/`Quantity`,2) AS `Sales_Per_Unit_Test`\")\r\n", "df6=df5.withColumn(\"Order_Date_Test\", when(isnull(col(\"Order Date\")) ,\"1900-01-01\").otherwise(date_format(to_date(col(\"Order Date\"),'dd/MM/yyyy'),\"yyyy-MM-dd\")))\r\n", "display(df6.limit(5))\r\n", "df6.printSchema()"], "outputs": []}, {"execution_count": 44, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["\"\"\"\r\n", "from notebookutils import mssparkutils \r\n", "mssparkutils.fs.mount(\r\n", "  'abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net',\r\n", "  '/test',\r\n", "  {'accountKey':'IPk7MdKGPWLF5i0Cf0kgvhT09mK73ZKMmouCyzJe2eDEKxPI3W/KZBibCq1V16IFgzCtpzUdxgIJ+ASteFvh+Q=='}\r\n", ")\r\n", "\"\"\""], "outputs": []}, {"execution_count": 45, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["#jobId=mssparkutils.env.getJobId()\r\n", "#df9= spark.read.option(\"header\",\"true\").csv('synfs:/'+jobId+ '/test/csv/Global Superstore.csv')\r\n", "#display(df9.limit(5))"], "outputs": []}, {"execution_count": 46, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["from datetime import date\r\n", "current_date = str(date.today())\r\n", "#mssparkutils.fs.mkdirs('synfs:/'+jobId+ '/test/Test_Output/'+current_date)\r\n", ""], "outputs": []}, {"execution_count": 47, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["\"\"\"\r\n", "import re\r\n", "\r\n", "csv_files = []\r\n", "max_run_id=\"0\"\r\n", "files_to_treat = mssparkutils.fs.ls('synfs:/'+jobId+ '/test/Test_Output/'+current_date)\r\n", "while files_to_treat:\r\n", "    path = files_to_treat.pop(0).path\r\n", "    if path.startswith('synfs:/'+jobId+ '/test/Test_Output/'+current_date+'/Test_Run_'):\r\n", "        if int(max_run_id)<int(path.replace('synfs:/'+jobId+ '/test/Test_Output/'+current_date+'/Test_Run_','')):\r\n", "            max_run_id=path.replace('synfs:/'+jobId+ '/test/Test_Output/'+current_date+'/Test_Run_','')\r\n", "\r\n", "new_run_id=str(int(max_run_id)+1)\r\n", "print(new_run_id)\r\n", "\r\n", "\"\"\"      \r\n", ""], "outputs": []}, {"execution_count": 48, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["#getMaximimRunIDfromPipelineSummaryTable\r\n", "\r\n", "pushdown_query = \"(Select * from sqldbschema.Pipeline_Summary) Pipeline_Summary\"\r\n", "df_pipleline_summ = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n", "df_pipleline_summ.registerTempTable('Pipeline_Summary')\r\n", "run_id=spark.sql(\"select ifnull(max(run_id),1) from Pipeline_Summary where layer='Raw' and run_date='\"+current_date+\"'\").first()[0]\r\n", "print(run_id)"], "outputs": []}, {"execution_count": 49, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["import pyspark.sql.functions as F\r\n", "\r\n", "\r\n", "df_combined_order_date = df6.filter((df6.Order_Date).eqNullSafe(df6.Order_Date_Test)==False).select(\"Row_ID\",\"Order_ID\",\"Order_Date\",\"Sales_Per_Unit\",\"Order_Priority\")\r\n", "order_date_error_count=df_combined_order_date.count()\r\n", "file_run_id=str(run_id)\r\n", "print(type(run_id))\r\n", "order_date_file_location='abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/Test_Output/'+current_date+'/Test_Run_'+file_run_id+'/Order_Date'\r\n", "if order_date_error_count>0:\r\n", "    df_combined_order_date.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").save(order_date_file_location)\r\n", "   \r\n", ""], "outputs": []}, {"execution_count": 50, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["df_combined_sales_pu = df6.filter(df6.Sales_Per_Unit!=df6.Sales_Per_Unit_Test).select(\"Row_ID\",\"Order_ID\",\"Order_Date\",\"Sales_Per_Unit\",\"Order_Priority\")\r\n", "sales_pu_error_count = df_combined_sales_pu.count()\r\n", "file_run_id=str(run_id)\r\n", "sales_pu_file_location=\"abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/Test_Output/\"+current_date+\"/Test_Run_\"+file_run_id+\"/Sales_PU\"\r\n", "print(sales_pu_file_location)\r\n", "if sales_pu_error_count>0:\r\n", "    df_combined_sales_pu.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").save(sales_pu_file_location)\r\n", "    "], "outputs": []}, {"execution_count": 51, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["\r\n", "\r\n", "\r\n", "\"\"\"\r\n", "spark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\r\n", "\r\n", "df_sales_pu=df6.select(\"Row_ID\",\"Sales_Per_Unit\",\"Sales_Per_Unit_Test\").collect()\r\n", "sales_pu_count=0\r\n", "#columns = [\"Row_ID\",\"Sales_Per_Unit\",\"Sales_Per_Unit_Test\"]\r\n", "columns = StructType([StructField(\"Row_ID\", StringType(), False),\r\n", "                      StructField(\"Sales_Per_Unit\", DoubleType(), False),\r\n", "                      StructField(\"Sales_Per_Unit_Test\", DoubleType(), False)])\r\n", "emp_RDD = spark_session.sparkContext.emptyRDD()\r\n", "\r\n", "df_combined_sales_pu = spark_session.createDataFrame(data=emp_RDD,schema=columns)\r\n", "file_run_id=str(new_run_id)\r\n", "sales_pu_file_location=\"abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/Test_Output/\"+current_date+\"/[\"+file_run_id+\"]_Sales_PU\"\r\n", "for row in df_sales_pu:\r\n", "  if row[\"Sales_Per_Unit\"]!=row[\"Sales_Per_Unit_Test\"]:\r\n", "    sales_pu_count=sales_pu_count+1\r\n", "    df_temp = spark.createDataFrame([(row[\"Row_ID\"],row[\"Sales_Per_Unit\"],row[\"Sales_Per_Unit_Test\"])], columns)\r\n", "    df_combined_sales_pu = df_combined_sales_pu.union(df_temp)\r\n", "\r\n", "display(df_combined_sales_pu.limit(5))\r\n", "\"\"\""], "outputs": []}, {"execution_count": 52, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["\"\"\"\r\n", "df_sales_pu=df6.select(\"Row_ID\",\"Sales_Per_Unit\",\"Sales_Per_Unit_Test\").collect()\r\n", "sales_pu_count=0\r\n", "columns = [\"Row_ID\",\"Sales_Per_Unit\",\"Sales_Per_Unit_Test\"]\r\n", "sales_pu_file_location=\"abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/Test_Output/\"+current_date+\"/[\"+str(new_run_id)+\"]_Sales_PU\"\r\n", "for row in df_sales_pu:\r\n", "  if row[\"Sales_Per_Unit\"]!=row[\"Sales_Per_Unit_Test\"]:\r\n", "    sales_pu_count=sales_pu_count+1\r\n", "    df_temp = spark.createDataFrame([(row[\"Row_ID\"],row[\"Sales_Per_Unit\"],row[\"Sales_Per_Unit_Test\"])], columns)\r\n", "    df_temp.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").save(sales_pu_file_location)\r\n", "    break\r\n", "\r\n", "print(sales_pu_count)\r\n", "\"\"\""], "outputs": []}, {"execution_count": 53, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["df_combined_order_priority = df6.filter(df6.Order_Priority!=df6.Order_Priority_Derived_Test).select(\"Row_ID\",\"Order_ID\",\"Order_Date\",\"Sales_Per_Unit\",\"Order_Priority\")\r\n", "order_priority_error_count = df_combined_order_priority.count()\r\n", "file_run_id=str(run_id)\r\n", "order_priority_file_location=\"abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/Test_Output/\"+current_date+\"/Test_Run_\"+file_run_id+\"/Order_Priority\"\r\n", "if order_priority_error_count>0:\r\n", "    df_combined_order_priority.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").save(order_priority_file_location)\r\n", "    "], "outputs": []}, {"execution_count": 54, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["columns = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "                      StructField(\"Run_Date\", StringType(), False),\r\n", "                      StructField(\"Run_ID\", IntegerType(), False),\r\n", "                      StructField(\"DataTransformation_TestScenario\", StringType(), False),\r\n", "                      StructField(\"Status\", StringType(), False),\r\n", "                      StructField(\"ErrorRecords\", IntegerType(), False),\r\n", "                      StructField(\"Path_To_Failed_Record_Dataset\", StringType(), False)])\r\n", "                      \r\n", "                    \r\n", "if sales_pu_error_count>0:\r\n", "    vals=[('global_superstore',current_date,run_id,'Sales PDU Data Format Check',\"Failed\",sales_pu_error_count,sales_pu_file_location.replace('abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/',''))]\r\n", "else:\r\n", "     vals=[('global_superstore',current_date,run_id,'Sales PDU Data Format Check',\"Success\",0,'')]\r\n", "\r\n", "df_temp_sales_pu = spark.createDataFrame(vals, columns) \r\n", ""], "outputs": []}, {"execution_count": 55, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["if order_date_error_count>0:\r\n", "    vals=[('global_superstore',current_date,run_id,'Date Change Transformation Check',\"Failed\",order_date_error_count,order_date_file_location.replace('abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/',''))]\r\n", "else:\r\n", "     vals=[('global_superstore',current_date,run_id,'Date Change Transformation Check',\"Success\",0,'')]\r\n", "\r\n", "df_temp_order_date = spark.createDataFrame(vals, columns) "], "outputs": []}, {"execution_count": 56, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["if order_priority_error_count>0:\r\n", "    vals=[('global_superstore',current_date,run_id,'Order Priority Check',\"Failed\",order_priority_error_count,order_priority_file_location.replace('abfss://etlstoragepocfilesystem@etlstoragepocaccount.dfs.core.windows.net/',''))]\r\n", "else:\r\n", "     vals=[('global_superstore',current_date,run_id,'Order Priority Check',\"Success\",0,'')]\r\n", "\r\n", "df_temp_order_priority = spark.createDataFrame(vals, columns) "], "outputs": []}, {"execution_count": 57, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}, "collapsed": false}, "source": ["#transformsummaryquery = \"(select * from  [sqldbschema].[Transformation_Summary]) Transformation_Summary\"\r\n", "#df_transformsummary = spark.read.jdbc(url=jdbcUrl, table=transformsummaryquery, properties=connectionProperties)\r\n", "#display(df_transformsummary.limit(5))\r\n", "\r\n", "df_transformsummary=df_temp_order_date.union(df_temp_sales_pu).union(df_temp_order_priority)\r\n", "display(df_transformsummary.limit(5))\r\n", ""], "outputs": []}, {"execution_count": 58, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["columns = StructType([StructField(\"Table_name\", StringType(), False),\r\n", "                      StructField(\"Run_Date\", StringType(), False),\r\n", "                      StructField(\"Run_ID\", IntegerType(), False),\r\n", "                      StructField(\"Layer\", StringType(), False),\r\n", "                      StructField(\"Status\", StringType(), False)])\r\n", "\r\n", "if (order_priority_error_count==0 and order_date_error_count==0 and sales_pu_error_count==0):\r\n", "    vals_pipelinesumm=[('global_superstore',current_date,run_id,'Transformation','Success')]\r\n", "    df_temp_pipelinesumm = spark.createDataFrame(vals_pipelinesumm, columns) \r\n", "else:\r\n", "     vals_pipelinesumm=[('global_superstore',current_date,run_id,'Transformation','Failed')]\r\n", "     df_temp_pipelinesumm = spark.createDataFrame(vals_pipelinesumm, columns) "], "outputs": []}, {"execution_count": 59, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["df_temp_pipelinesumm.write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Pipeline_Summary\", properties=connectionProperties)"], "outputs": []}, {"execution_count": 61, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["df_transformsummary.write.mode('append').jdbc(jdbcUrl,\"sqldbschema.Transformation_Summary\", properties=connectionProperties)"], "outputs": []}, {"execution_count": 1, "cell_type": "code", "metadata": {"jupyter": {"source_hidden": false, "outputs_hidden": false}, "nteract": {"transient": {"deleting": false}}}, "source": ["return_var=\"0\"\r\n", "\r\n", "if (order_priority_error_count==0 and order_date_error_count==0 and sales_pu_error_count==0):\r\n", "    return_val=\"1\"\r\n", "\r\n", "mssparkutils.notebook.exit(return_var)\r\n", ""], "outputs": []}]}}